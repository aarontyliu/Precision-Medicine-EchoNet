{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import EchoNetDataset\n",
    "from models import EchoNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory = torch.rand(10, 32, 512)\n",
    "tgt = torch.rand(20, 32, 512)\n",
    "out = transformer_decoder(tgt, memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(num_output_channels=1)\n",
    "])\n",
    "\n",
    "trainset = EchoNetDataset(\n",
    "    root_dir='/home/tienyu/data/EchoNet-Dynamic/Videos',\n",
    "    target_csv='/home/tienyu/data/EchoNet-Dynamic/FileList.csv',\n",
    "    split='train',\n",
    "    transform=transform)\n",
    "\n",
    "valset = EchoNetDataset(\n",
    "    root_dir='/home/tienyu/data/EchoNet-Dynamic/Videos',\n",
    "    target_csv='/home/tienyu/data/EchoNet-Dynamic/FileList.csv',\n",
    "    split='val',\n",
    "    transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size)\n",
    "valloader = DataLoader(valset, batch_size=batch_size)\n",
    "dataloaders = {'train': trainloader, 'val': valloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EchoNetClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Iteration: 2 \t Train Loss: 12218.51660\n",
      "Iteration: 4 \t Train Loss: 46643.68530\n",
      "Iteration: 6 \t Train Loss: 35244.73438\n",
      "Iteration: 8 \t Train Loss: 32137.28113\n",
      "Iteration: 10 \t Train Loss: 33280.14111\n",
      "Iteration: 12 \t Train Loss: 28611.50863\n",
      "Iteration: 14 \t Train Loss: 25236.39701\n",
      "Iteration: 16 \t Train Loss: 22833.48429\n",
      "Iteration: 18 \t Train Loss: 20782.60288\n",
      "Iteration: 20 \t Train Loss: 18980.65771\n",
      "Iteration: 22 \t Train Loss: 18912.84528\n",
      "Iteration: 24 \t Train Loss: 17686.24466\n",
      "Iteration: 26 \t Train Loss: 16590.17180\n",
      "Iteration: 28 \t Train Loss: 15765.97881\n",
      "Iteration: 30 \t Train Loss: 14851.45490\n",
      "Iteration: 32 \t Train Loss: 14714.24643\n",
      "Iteration: 34 \t Train Loss: 14060.70674\n",
      "Iteration: 36 \t Train Loss: 13498.31698\n",
      "Iteration: 38 \t Train Loss: 12923.44711\n",
      "Iteration: 40 \t Train Loss: 12458.52292\n",
      "Iteration: 42 \t Train Loss: 12101.52401\n",
      "Iteration: 44 \t Train Loss: 11693.11684\n",
      "Iteration: 46 \t Train Loss: 11300.14861\n",
      "Iteration: 48 \t Train Loss: 10902.97961\n",
      "Iteration: 50 \t Train Loss: 10988.70363\n",
      "Iteration: 52 \t Train Loss: 10699.32376\n",
      "Iteration: 54 \t Train Loss: 10403.93934\n",
      "Iteration: 56 \t Train Loss: 10176.01610\n",
      "Iteration: 58 \t Train Loss: 9913.91250\n",
      "Iteration: 60 \t Train Loss: 9744.45059\n",
      "Iteration: 62 \t Train Loss: 9506.55522\n",
      "Iteration: 64 \t Train Loss: 9279.32281\n",
      "Iteration: 66 \t Train Loss: 9136.33057\n",
      "Iteration: 68 \t Train Loss: 8910.29838\n",
      "Iteration: 70 \t Train Loss: 8808.89828\n",
      "Iteration: 72 \t Train Loss: 8651.86764\n",
      "Iteration: 74 \t Train Loss: 8538.37409\n",
      "Iteration: 76 \t Train Loss: 8503.26294\n",
      "Iteration: 78 \t Train Loss: 8328.87505\n",
      "Iteration: 80 \t Train Loss: 8167.88962\n",
      "Iteration: 82 \t Train Loss: 8011.91162\n",
      "Iteration: 84 \t Train Loss: 7892.28376\n",
      "Iteration: 86 \t Train Loss: 7744.78149\n",
      "Iteration: 88 \t Train Loss: 7678.65703\n",
      "Iteration: 90 \t Train Loss: 7596.03926\n",
      "Iteration: 92 \t Train Loss: 7474.57553\n",
      "Iteration: 94 \t Train Loss: 7344.54512\n",
      "Iteration: 96 \t Train Loss: 7228.73808\n",
      "Iteration: 98 \t Train Loss: 7110.61177\n",
      "Iteration: 100 \t Train Loss: 7024.15468\n",
      "Iteration: 102 \t Train Loss: 6996.64423\n",
      "Iteration: 104 \t Train Loss: 6921.02297\n",
      "Iteration: 106 \t Train Loss: 6944.59507\n",
      "Iteration: 108 \t Train Loss: 6841.25559\n",
      "Iteration: 110 \t Train Loss: 6742.22071\n",
      "Iteration: 112 \t Train Loss: 6772.78298\n",
      "Iteration: 114 \t Train Loss: 6759.29837\n",
      "Iteration: 116 \t Train Loss: 6748.53170\n",
      "Iteration: 118 \t Train Loss: 6716.48492\n",
      "Iteration: 120 \t Train Loss: 6666.60612\n",
      "Iteration: 122 \t Train Loss: 6600.91717\n",
      "Iteration: 124 \t Train Loss: 6519.15765\n",
      "Iteration: 126 \t Train Loss: 6452.18208\n",
      "Iteration: 128 \t Train Loss: 6379.30562\n",
      "Iteration: 130 \t Train Loss: 6435.77100\n",
      "Iteration: 132 \t Train Loss: 6370.46422\n",
      "Iteration: 134 \t Train Loss: 6297.20943\n",
      "Iteration: 136 \t Train Loss: 6246.35932\n",
      "Iteration: 138 \t Train Loss: 6165.32308\n",
      "Iteration: 140 \t Train Loss: 6101.88528\n",
      "Iteration: 142 \t Train Loss: 6118.86537\n",
      "Iteration: 144 \t Train Loss: 6051.11247\n",
      "Iteration: 146 \t Train Loss: 5998.63051\n",
      "Iteration: 148 \t Train Loss: 6010.77841\n",
      "Iteration: 150 \t Train Loss: 5957.49080\n",
      "Iteration: 152 \t Train Loss: 5913.14476\n",
      "Iteration: 154 \t Train Loss: 5893.50845\n",
      "Iteration: 156 \t Train Loss: 5923.60739\n",
      "Iteration: 158 \t Train Loss: 5887.00562\n",
      "Iteration: 160 \t Train Loss: 5914.88208\n",
      "Iteration: 162 \t Train Loss: 5862.71900\n",
      "Iteration: 164 \t Train Loss: 5895.72623\n",
      "Iteration: 166 \t Train Loss: 5849.35074\n",
      "Iteration: 168 \t Train Loss: 5828.20274\n",
      "Iteration: 170 \t Train Loss: 5775.41490\n",
      "Iteration: 172 \t Train Loss: 5735.50141\n",
      "Iteration: 174 \t Train Loss: 5703.33852\n",
      "Iteration: 176 \t Train Loss: 5659.49829\n",
      "Iteration: 178 \t Train Loss: 5611.84546\n",
      "Iteration: 180 \t Train Loss: 5566.50473\n",
      "Iteration: 182 \t Train Loss: 5520.72654\n",
      "Iteration: 184 \t Train Loss: 5575.05506\n",
      "Iteration: 186 \t Train Loss: 5648.19091\n",
      "Iteration: 188 \t Train Loss: 5610.49689\n",
      "Iteration: 190 \t Train Loss: 5584.74804\n",
      "Iteration: 192 \t Train Loss: 5554.59597\n",
      "Iteration: 194 \t Train Loss: 5521.46829\n",
      "Iteration: 196 \t Train Loss: 5492.14412\n",
      "Iteration: 198 \t Train Loss: 5457.68510\n",
      "Iteration: 200 \t Train Loss: 5421.58234\n",
      "Iteration: 202 \t Train Loss: 5450.18356\n",
      "Iteration: 204 \t Train Loss: 5416.22653\n",
      "Iteration: 206 \t Train Loss: 5393.57017\n",
      "Iteration: 208 \t Train Loss: 5364.73742\n",
      "Iteration: 210 \t Train Loss: 5328.10306\n",
      "Iteration: 212 \t Train Loss: 5384.94424\n",
      "Iteration: 214 \t Train Loss: 5385.15502\n",
      "Iteration: 216 \t Train Loss: 5360.77618\n",
      "Iteration: 218 \t Train Loss: 5332.07653\n",
      "Iteration: 220 \t Train Loss: 5321.95568\n",
      "Iteration: 222 \t Train Loss: 5297.02489\n",
      "Iteration: 224 \t Train Loss: 5283.07579\n",
      "Iteration: 226 \t Train Loss: 5258.61960\n",
      "Iteration: 228 \t Train Loss: 5237.15499\n",
      "Iteration: 230 \t Train Loss: 5212.99364\n",
      "Iteration: 232 \t Train Loss: 5216.52018\n",
      "Iteration: 234 \t Train Loss: 5187.81081\n",
      "Iteration: 236 \t Train Loss: 5263.69507\n",
      "Iteration: 238 \t Train Loss: 5254.78958\n",
      "Iteration: 240 \t Train Loss: 5248.77722\n",
      "Iteration: 242 \t Train Loss: 5231.39895\n",
      "Iteration: 244 \t Train Loss: 5217.98457\n",
      "Iteration: 246 \t Train Loss: 5192.11735\n",
      "Iteration: 248 \t Train Loss: 5173.03835\n",
      "Iteration: 250 \t Train Loss: 5143.05108\n",
      "Iteration: 252 \t Train Loss: 5147.10355\n",
      "Iteration: 254 \t Train Loss: 5117.67406\n",
      "Iteration: 256 \t Train Loss: 5080.32784\n",
      "Iteration: 258 \t Train Loss: 5069.45390\n",
      "Iteration: 260 \t Train Loss: 5122.11933\n",
      "Iteration: 262 \t Train Loss: 5093.07991\n",
      "Iteration: 264 \t Train Loss: 5081.71270\n",
      "Iteration: 266 \t Train Loss: 5056.38639\n",
      "Iteration: 268 \t Train Loss: 5028.53095\n",
      "Iteration: 270 \t Train Loss: 5001.45194\n",
      "Iteration: 272 \t Train Loss: 5141.08503\n",
      "Iteration: 274 \t Train Loss: 5112.03761\n",
      "Iteration: 276 \t Train Loss: 5086.10352\n",
      "Iteration: 278 \t Train Loss: 5155.55996\n",
      "Iteration: 280 \t Train Loss: 5136.08337\n",
      "Iteration: 282 \t Train Loss: 5134.47624\n",
      "Iteration: 284 \t Train Loss: 5148.96245\n",
      "Iteration: 286 \t Train Loss: 5126.82475\n",
      "Iteration: 288 \t Train Loss: 5101.23468\n",
      "Iteration: 290 \t Train Loss: 5099.03534\n",
      "Iteration: 292 \t Train Loss: 5098.51912\n",
      "Iteration: 294 \t Train Loss: 5076.29356\n",
      "Iteration: 296 \t Train Loss: 5051.96367\n",
      "Iteration: 298 \t Train Loss: 5040.89553\n",
      "Iteration: 300 \t Train Loss: 5056.81637\n",
      "Iteration: 302 \t Train Loss: 5030.86245\n",
      "Iteration: 304 \t Train Loss: 5019.46976\n",
      "Iteration: 306 \t Train Loss: 5002.22713\n",
      "Iteration: 308 \t Train Loss: 5024.91710\n",
      "Iteration: 310 \t Train Loss: 5068.99867\n",
      "Iteration: 312 \t Train Loss: 5046.28663\n",
      "Iteration: 314 \t Train Loss: 5026.55829\n",
      "Iteration: 316 \t Train Loss: 5023.76908\n",
      "Iteration: 318 \t Train Loss: 5007.66606\n",
      "Iteration: 320 \t Train Loss: 4989.86421\n",
      "Iteration: 322 \t Train Loss: 5021.54666\n",
      "Iteration: 324 \t Train Loss: 5020.72621\n",
      "Iteration: 326 \t Train Loss: 4999.01877\n",
      "Iteration: 328 \t Train Loss: 4972.95201\n",
      "Iteration: 330 \t Train Loss: 4949.48413\n",
      "Iteration: 332 \t Train Loss: 4954.91862\n",
      "Iteration: 334 \t Train Loss: 4949.31974\n",
      "Iteration: 336 \t Train Loss: 4946.45031\n",
      "Iteration: 338 \t Train Loss: 4941.05136\n",
      "Iteration: 340 \t Train Loss: 4929.54834\n",
      "Iteration: 342 \t Train Loss: 5127.73212\n",
      "Iteration: 344 \t Train Loss: 5112.22650\n",
      "Iteration: 346 \t Train Loss: 5099.71648\n",
      "Iteration: 348 \t Train Loss: 5087.83141\n",
      "Iteration: 350 \t Train Loss: 5080.89239\n",
      "Iteration: 352 \t Train Loss: 5127.60268\n",
      "Iteration: 354 \t Train Loss: 5121.25255\n",
      "Iteration: 356 \t Train Loss: 5110.39009\n",
      "Iteration: 358 \t Train Loss: 5099.20671\n",
      "Iteration: 360 \t Train Loss: 5082.50444\n",
      "Iteration: 362 \t Train Loss: 5067.32398\n",
      "Iteration: 364 \t Train Loss: 5048.52315\n",
      "Iteration: 366 \t Train Loss: 5030.12145\n",
      "Iteration: 368 \t Train Loss: 5027.75774\n",
      "Iteration: 370 \t Train Loss: 5025.11349\n",
      "Iteration: 372 \t Train Loss: 5026.96079\n",
      "Iteration: 374 \t Train Loss: 5014.88249\n",
      "Iteration: 376 \t Train Loss: 4996.35915\n",
      "Iteration: 378 \t Train Loss: 5001.75430\n",
      "Iteration: 380 \t Train Loss: 4986.41206\n",
      "Iteration: 382 \t Train Loss: 4984.13283\n",
      "Iteration: 384 \t Train Loss: 4969.24367\n",
      "Iteration: 386 \t Train Loss: 4953.90769\n",
      "Iteration: 388 \t Train Loss: 4934.85955\n",
      "Iteration: 390 \t Train Loss: 4932.57537\n",
      "Iteration: 392 \t Train Loss: 4911.95972\n",
      "Iteration: 394 \t Train Loss: 4891.52611\n",
      "Iteration: 396 \t Train Loss: 4874.15701\n",
      "Iteration: 398 \t Train Loss: 4867.50501\n",
      "Iteration: 400 \t Train Loss: 4893.95793\n",
      "Iteration: 402 \t Train Loss: 4877.85801\n",
      "Iteration: 404 \t Train Loss: 4881.73210\n",
      "Iteration: 406 \t Train Loss: 4888.37492\n",
      "Iteration: 408 \t Train Loss: 4877.07737\n",
      "Iteration: 410 \t Train Loss: 4863.72489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 412 \t Train Loss: 4885.09608\n",
      "Iteration: 414 \t Train Loss: 4876.68102\n",
      "Iteration: 416 \t Train Loss: 4870.40767\n",
      "Iteration: 418 \t Train Loss: 4854.71860\n",
      "Iteration: 420 \t Train Loss: 4843.16856\n",
      "Iteration: 422 \t Train Loss: 4830.41221\n",
      "Iteration: 424 \t Train Loss: 4819.28146\n",
      "Iteration: 426 \t Train Loss: 4809.22326\n",
      "Iteration: 428 \t Train Loss: 4797.05126\n",
      "Iteration: 430 \t Train Loss: 4786.92799\n",
      "Iteration: 432 \t Train Loss: 4779.35573\n",
      "Iteration: 434 \t Train Loss: 4770.23658\n",
      "Iteration: 436 \t Train Loss: 4758.51272\n",
      "Iteration: 438 \t Train Loss: 4742.13389\n",
      "Iteration: 440 \t Train Loss: 4768.48353\n",
      "Iteration: 442 \t Train Loss: 4762.12008\n",
      "Iteration: 444 \t Train Loss: 4766.53521\n",
      "Iteration: 446 \t Train Loss: 4755.03962\n",
      "Iteration: 448 \t Train Loss: 4742.83724\n",
      "Iteration: 450 \t Train Loss: 4728.68711\n",
      "Iteration: 452 \t Train Loss: 4734.15988\n",
      "Iteration: 454 \t Train Loss: 4720.81038\n",
      "Iteration: 456 \t Train Loss: 4714.45873\n",
      "Iteration: 458 \t Train Loss: 4699.27715\n",
      "Iteration: 460 \t Train Loss: 4691.77195\n",
      "Iteration: 462 \t Train Loss: 4682.77088\n",
      "Iteration: 464 \t Train Loss: 4674.65444\n",
      "Iteration: 466 \t Train Loss: 4680.01420\n",
      "Iteration: 468 \t Train Loss: 4673.27289\n",
      "Iteration: 470 \t Train Loss: 4660.90371\n",
      "Iteration: 472 \t Train Loss: 4655.13660\n",
      "Iteration: 474 \t Train Loss: 4646.49319\n",
      "Iteration: 476 \t Train Loss: 4632.71790\n",
      "Iteration: 478 \t Train Loss: 4621.05986\n",
      "Iteration: 480 \t Train Loss: 4645.08139\n",
      "Iteration: 482 \t Train Loss: 4631.43285\n",
      "Iteration: 484 \t Train Loss: 4617.66880\n",
      "Iteration: 486 \t Train Loss: 4613.27662\n",
      "Iteration: 488 \t Train Loss: 4599.77420\n",
      "Iteration: 490 \t Train Loss: 4587.14294\n",
      "Iteration: 492 \t Train Loss: 4577.47191\n",
      "Iteration: 494 \t Train Loss: 4599.17967\n",
      "Iteration: 496 \t Train Loss: 4636.71945\n",
      "Iteration: 498 \t Train Loss: 4626.11545\n",
      "Iteration: 500 \t Train Loss: 4620.58852\n",
      "Iteration: 502 \t Train Loss: 4622.98767\n",
      "Iteration: 504 \t Train Loss: 4614.60592\n",
      "Iteration: 506 \t Train Loss: 4602.49028\n",
      "Iteration: 508 \t Train Loss: 4594.69279\n",
      "Iteration: 510 \t Train Loss: 4583.91429\n",
      "Iteration: 512 \t Train Loss: 4573.08796\n",
      "Iteration: 514 \t Train Loss: 4562.84614\n",
      "Iteration: 516 \t Train Loss: 4585.59444\n",
      "Iteration: 518 \t Train Loss: 4576.34634\n",
      "Iteration: 520 \t Train Loss: 4579.62222\n",
      "Iteration: 522 \t Train Loss: 4570.52936\n",
      "Iteration: 524 \t Train Loss: 4565.68527\n",
      "Iteration: 526 \t Train Loss: 4575.90130\n",
      "Iteration: 528 \t Train Loss: 4575.34169\n",
      "Iteration: 530 \t Train Loss: 4571.94403\n",
      "Iteration: 532 \t Train Loss: 4560.55005\n",
      "Iteration: 534 \t Train Loss: 4548.20406\n",
      "Iteration: 536 \t Train Loss: 4537.41745\n",
      "Iteration: 538 \t Train Loss: 4525.32778\n",
      "Iteration: 540 \t Train Loss: 4513.51466\n",
      "Iteration: 542 \t Train Loss: 4544.96381\n",
      "Iteration: 544 \t Train Loss: 4540.20589\n",
      "Iteration: 546 \t Train Loss: 4531.63502\n",
      "Iteration: 548 \t Train Loss: 4525.09645\n",
      "Iteration: 550 \t Train Loss: 4515.84653\n",
      "Iteration: 552 \t Train Loss: 4540.84313\n",
      "Iteration: 554 \t Train Loss: 4531.52821\n",
      "Iteration: 556 \t Train Loss: 4522.29424\n",
      "Iteration: 558 \t Train Loss: 4511.95027\n",
      "Iteration: 560 \t Train Loss: 4510.26558\n",
      "Iteration: 562 \t Train Loss: 4502.88364\n",
      "Iteration: 564 \t Train Loss: 4493.61567\n",
      "Iteration: 566 \t Train Loss: 4506.02605\n",
      "Iteration: 568 \t Train Loss: 4498.59743\n",
      "Iteration: 570 \t Train Loss: 4521.78229\n",
      "Iteration: 572 \t Train Loss: 4512.17425\n",
      "Iteration: 574 \t Train Loss: 4506.32228\n",
      "Iteration: 576 \t Train Loss: 4497.15676\n",
      "Iteration: 578 \t Train Loss: 4501.59004\n",
      "Iteration: 580 \t Train Loss: 4517.88784\n",
      "Iteration: 582 \t Train Loss: 4509.37469\n",
      "Iteration: 584 \t Train Loss: 4515.16940\n",
      "Iteration: 586 \t Train Loss: 4506.21134\n",
      "Iteration: 588 \t Train Loss: 4500.73484\n",
      "Iteration: 590 \t Train Loss: 4494.06880\n",
      "Iteration: 592 \t Train Loss: 4490.68659\n",
      "Iteration: 594 \t Train Loss: 4495.62686\n",
      "Iteration: 596 \t Train Loss: 4486.64147\n",
      "Iteration: 598 \t Train Loss: 4480.89445\n",
      "Iteration: 600 \t Train Loss: 4471.97653\n",
      "Iteration: 602 \t Train Loss: 4463.25771\n",
      "Iteration: 604 \t Train Loss: 4459.29645\n",
      "Iteration: 606 \t Train Loss: 4458.38406\n",
      "Iteration: 608 \t Train Loss: 4446.18055\n",
      "Iteration: 610 \t Train Loss: 4438.81415\n",
      "Iteration: 612 \t Train Loss: 4428.02258\n",
      "Iteration: 614 \t Train Loss: 4418.78108\n",
      "Iteration: 616 \t Train Loss: 4423.91209\n",
      "Iteration: 618 \t Train Loss: 4414.84178\n",
      "Iteration: 620 \t Train Loss: 4405.90057\n",
      "Iteration: 622 \t Train Loss: 4399.06956\n",
      "Iteration: 624 \t Train Loss: 4391.02260\n",
      "Iteration: 626 \t Train Loss: 4381.71773\n",
      "Iteration: 628 \t Train Loss: 4370.07903\n",
      "Iteration: 630 \t Train Loss: 4363.83434\n",
      "Iteration: 632 \t Train Loss: 4363.76816\n",
      "Iteration: 634 \t Train Loss: 4357.32566\n",
      "Iteration: 636 \t Train Loss: 4347.53646\n",
      "Iteration: 638 \t Train Loss: 4344.66018\n",
      "Iteration: 640 \t Train Loss: 4364.97194\n",
      "Iteration: 642 \t Train Loss: 4358.60087\n",
      "Iteration: 644 \t Train Loss: 4350.94577\n",
      "Iteration: 646 \t Train Loss: 4342.68241\n",
      "Iteration: 648 \t Train Loss: 4341.99308\n",
      "Iteration: 650 \t Train Loss: 4340.33865\n",
      "Iteration: 652 \t Train Loss: 4340.32653\n",
      "Iteration: 654 \t Train Loss: 4345.59209\n",
      "Iteration: 656 \t Train Loss: 4341.68046\n",
      "Iteration: 658 \t Train Loss: 4332.13344\n",
      "Iteration: 660 \t Train Loss: 4323.90823\n",
      "Iteration: 662 \t Train Loss: 4335.69390\n",
      "Iteration: 664 \t Train Loss: 4328.43437\n",
      "Iteration: 666 \t Train Loss: 4320.55402\n",
      "Iteration: 668 \t Train Loss: 4313.74350\n",
      "Iteration: 670 \t Train Loss: 4304.58095\n",
      "Iteration: 672 \t Train Loss: 4307.01652\n",
      "Iteration: 674 \t Train Loss: 4296.86599\n",
      "Iteration: 676 \t Train Loss: 4289.14161\n",
      "Iteration: 678 \t Train Loss: 4278.63436\n",
      "Iteration: 680 \t Train Loss: 4278.38424\n",
      "Iteration: 682 \t Train Loss: 4274.30711\n",
      "Iteration: 684 \t Train Loss: 4276.76813\n",
      "Iteration: 686 \t Train Loss: 4267.23242\n",
      "Iteration: 688 \t Train Loss: 4270.91322\n",
      "Iteration: 690 \t Train Loss: 4278.66227\n",
      "Iteration: 692 \t Train Loss: 4274.31725\n",
      "Iteration: 694 \t Train Loss: 4270.82630\n",
      "Iteration: 696 \t Train Loss: 4262.42821\n",
      "Iteration: 698 \t Train Loss: 4254.29413\n",
      "Iteration: 700 \t Train Loss: 4247.65460\n",
      "Iteration: 702 \t Train Loss: 4257.91715\n",
      "Iteration: 704 \t Train Loss: 4250.87048\n",
      "Iteration: 706 \t Train Loss: 4244.44827\n",
      "Iteration: 708 \t Train Loss: 4237.04272\n",
      "Iteration: 710 \t Train Loss: 4227.97698\n",
      "Iteration: 712 \t Train Loss: 4221.51593\n",
      "Iteration: 714 \t Train Loss: 4217.04560\n",
      "Iteration: 716 \t Train Loss: 4215.58604\n",
      "Iteration: 718 \t Train Loss: 4208.95776\n",
      "Iteration: 720 \t Train Loss: 4216.17597\n",
      "Iteration: 722 \t Train Loss: 4213.87844\n",
      "Iteration: 724 \t Train Loss: 4208.58320\n",
      "Iteration: 726 \t Train Loss: 4212.34816\n",
      "Iteration: 728 \t Train Loss: 4219.58464\n",
      "Iteration: 730 \t Train Loss: 4219.52764\n",
      "Iteration: 732 \t Train Loss: 4223.27547\n",
      "Iteration: 734 \t Train Loss: 4220.57464\n",
      "Iteration: 736 \t Train Loss: 4214.55076\n",
      "Iteration: 738 \t Train Loss: 4212.34403\n",
      "Iteration: 740 \t Train Loss: 4205.47081\n",
      "Iteration: 742 \t Train Loss: 4197.06139\n",
      "Iteration: 744 \t Train Loss: 4188.61132\n",
      "Iteration: 746 \t Train Loss: 4183.81370\n",
      "Iteration: 748 \t Train Loss: 4178.58687\n",
      "Iteration: 750 \t Train Loss: 4171.94454\n",
      "Iteration: 752 \t Train Loss: 4165.94491\n",
      "Iteration: 754 \t Train Loss: 4170.38993\n",
      "Iteration: 756 \t Train Loss: 4181.71260\n",
      "Iteration: 758 \t Train Loss: 4176.22008\n",
      "Iteration: 760 \t Train Loss: 4168.35602\n",
      "Iteration: 762 \t Train Loss: 4162.44638\n",
      "Iteration: 764 \t Train Loss: 4159.14775\n",
      "Iteration: 766 \t Train Loss: 4150.81971\n",
      "Iteration: 768 \t Train Loss: 4151.21822\n",
      "Iteration: 770 \t Train Loss: 4145.56833\n",
      "Iteration: 772 \t Train Loss: 4142.43815\n",
      "Iteration: 774 \t Train Loss: 4141.84891\n",
      "Iteration: 776 \t Train Loss: 4147.50517\n",
      "Iteration: 778 \t Train Loss: 4141.99426\n",
      "Iteration: 780 \t Train Loss: 4136.14391\n",
      "Iteration: 782 \t Train Loss: 4128.59835\n",
      "Iteration: 784 \t Train Loss: 4122.63322\n",
      "Iteration: 786 \t Train Loss: 4122.27817\n",
      "Iteration: 788 \t Train Loss: 4233.57515\n",
      "Iteration: 790 \t Train Loss: 4227.72847\n",
      "Iteration: 792 \t Train Loss: 4232.64460\n",
      "Iteration: 794 \t Train Loss: 4226.48428\n",
      "Iteration: 796 \t Train Loss: 4223.07118\n",
      "Iteration: 798 \t Train Loss: 4217.21044\n",
      "Iteration: 800 \t Train Loss: 4211.51861\n",
      "Iteration: 802 \t Train Loss: 4203.41639\n",
      "Iteration: 804 \t Train Loss: 4197.00036\n",
      "Iteration: 806 \t Train Loss: 4191.12538\n",
      "Iteration: 808 \t Train Loss: 4193.53676\n",
      "Iteration: 810 \t Train Loss: 4186.88286\n",
      "Iteration: 812 \t Train Loss: 4190.72442\n",
      "Iteration: 814 \t Train Loss: 4183.19421\n",
      "Iteration: 816 \t Train Loss: 4183.85895\n",
      "Iteration: 818 \t Train Loss: 4183.84748\n",
      "Iteration: 820 \t Train Loss: 4178.71890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 822 \t Train Loss: 4175.83640\n",
      "Iteration: 824 \t Train Loss: 4184.36739\n",
      "Iteration: 826 \t Train Loss: 4179.91276\n",
      "Iteration: 828 \t Train Loss: 4180.57901\n",
      "Iteration: 830 \t Train Loss: 4175.46537\n",
      "Iteration: 832 \t Train Loss: 4169.51487\n",
      "Iteration: 834 \t Train Loss: 4174.07842\n",
      "Iteration: 836 \t Train Loss: 4170.49321\n",
      "Iteration: 838 \t Train Loss: 4170.39189\n",
      "Iteration: 840 \t Train Loss: 4165.79539\n",
      "Iteration: 842 \t Train Loss: 4159.20643\n",
      "Iteration: 844 \t Train Loss: 4161.91489\n",
      "Iteration: 846 \t Train Loss: 4165.43678\n",
      "Iteration: 848 \t Train Loss: 4159.74951\n",
      "Iteration: 850 \t Train Loss: 4153.46288\n",
      "Iteration: 852 \t Train Loss: 4155.94159\n",
      "Iteration: 854 \t Train Loss: 4150.68469\n",
      "Iteration: 856 \t Train Loss: 4149.28458\n",
      "Iteration: 858 \t Train Loss: 4156.91098\n",
      "Iteration: 860 \t Train Loss: 4153.49954\n",
      "Iteration: 862 \t Train Loss: 4170.67049\n",
      "Iteration: 864 \t Train Loss: 4170.97145\n",
      "Iteration: 866 \t Train Loss: 4165.28885\n",
      "Iteration: 868 \t Train Loss: 4161.23976\n",
      "Iteration: 870 \t Train Loss: 4164.44651\n",
      "Iteration: 872 \t Train Loss: 4172.20909\n",
      "Iteration: 874 \t Train Loss: 4180.00531\n",
      "Iteration: 876 \t Train Loss: 4175.48811\n",
      "Iteration: 878 \t Train Loss: 4192.11774\n",
      "Iteration: 880 \t Train Loss: 4186.91929\n",
      "Iteration: 882 \t Train Loss: 4187.03647\n",
      "Iteration: 884 \t Train Loss: 4182.48146\n",
      "Iteration: 886 \t Train Loss: 4176.74822\n",
      "Iteration: 888 \t Train Loss: 4173.52148\n",
      "Iteration: 890 \t Train Loss: 4167.30921\n",
      "Iteration: 892 \t Train Loss: 4161.43579\n",
      "Iteration: 894 \t Train Loss: 4155.84726\n",
      "Iteration: 896 \t Train Loss: 4151.99806\n",
      "Iteration: 898 \t Train Loss: 4143.91693\n",
      "Iteration: 900 \t Train Loss: 4151.41497\n",
      "Iteration: 902 \t Train Loss: 4149.86970\n",
      "Iteration: 904 \t Train Loss: 4152.03269\n",
      "Iteration: 906 \t Train Loss: 4145.64690\n",
      "Iteration: 908 \t Train Loss: 4140.12711\n",
      "Iteration: 910 \t Train Loss: 4135.35069\n",
      "Iteration: 912 \t Train Loss: 4129.63336\n",
      "Iteration: 914 \t Train Loss: 4124.08888\n",
      "Iteration: 916 \t Train Loss: 4117.62383\n",
      "Iteration: 918 \t Train Loss: 4113.00408\n",
      "Iteration: 920 \t Train Loss: 4113.21018\n",
      "Iteration: 922 \t Train Loss: 4107.08505\n",
      "Iteration: 924 \t Train Loss: 4101.01034\n",
      "Iteration: 926 \t Train Loss: 4096.72937\n",
      "Iteration: 928 \t Train Loss: 4106.08635\n",
      "Iteration: 930 \t Train Loss: 4114.41298\n",
      "Iteration: 932 \t Train Loss: 4113.66804\n",
      "Iteration: 934 \t Train Loss: 4106.59231\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-761ffaa4f2a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 )\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             print(\n\u001b[1;32m     44\u001b[0m                 \u001b[0;34mf\"Iteration: {iterations:.0f} \\t {phase.title()} Loss: {val_loss:.5f}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loaders' is not defined"
     ]
    }
   ],
   "source": [
    "best_val_loss = torch.finfo(torch.float32).max\n",
    "since = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    iterations = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for phase in ('train', 'val'):\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        for video_tensor, labels, nofs in dataloaders[phase]:\n",
    "            iterations += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ef, esv, edv = [l.float().to(device) for l in labels]\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(video_tensor)\n",
    "                esv_pred, edv_pred = outputs[nofs - 1, range(len(nofs)), :].T\n",
    "\n",
    "                loss_esv = criterion(esv_pred, esv)\n",
    "                loss_edv = criterion(edv_pred, edv)\n",
    "                ef_pred = torch.clip(100 * (edv_pred - esv_pred) / (edv_pred + 1e-5), min=0., max=100.)\n",
    "                loss_ef = criterion(ef_pred, ef)\n",
    "\n",
    "                total_loss = loss_esv + loss_edv + loss_ef\n",
    "                if phase == 'train':\n",
    "                    total_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += total_loss.item() * video_tensor.size(0)\n",
    "\n",
    "            if not iterations % n_iter and phase == 'train':\n",
    "                print(\n",
    "                    f\"Iteration: {iterations:.0f} \\t {phase.title()} Loss: {running_loss/(iterations*batch_size):.5f}\"\n",
    "                )\n",
    "        if phase == 'val':\n",
    "            val_loss = running_loss/len(dataloaders[phase].dataset)\n",
    "            print(\n",
    "                f\"Iteration: {iterations:.0f} \\t {phase.title()} Loss: {val_loss:.5f}\"\n",
    "            )\n",
    "            if val_loss < best_val_loss:\n",
    "                torch.save(model.state_dict(), 'checkpoints/best_checkpoint.pt')\n",
    "            best_val_loss = val_loss\n",
    "    print()\n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import av\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EchoNetDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "    def __init__(self,\n",
    "                 target_csv,\n",
    "                 root_dir,\n",
    "                 split=\"train\",\n",
    "                 transform=None,\n",
    "                 frame_size=112,\n",
    "                 sampling_frequency=4,\n",
    "                 clip_length=16):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            target_csv (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        assert split in (\n",
    "            \"train\",\n",
    "            \"val\",\n",
    "            \"test\",\n",
    "        ), \"Please validate the split specification (train, val or test)\"\n",
    "        self.split = split.upper()\n",
    "        self.sampling_frequency = sampling_frequency\n",
    "        self.clip_length = clip_length\n",
    "        self.min_num_frames = self.clip_length * self.sampling_frequency\n",
    "        self.df = pd.read_csv(target_csv)\n",
    "        self.df = self.df.loc[(self.df[\"Split\"] == self.split) & (self.df['NumberOfFrames'] > self.min_num_frames)]\n",
    "        self.max_num_frames = self.df.NumberOfFrames.max()\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.frame_size = frame_size\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        video_name = os.path.join(self.root_dir, self.df.iloc[idx, 0]) + \".avi\"\n",
    "        container = av.open(video_name)\n",
    "\n",
    "        frames = []\n",
    "        for frame in container.decode(video=0):\n",
    "            f = frame.to_image()\n",
    "            if self.transform:\n",
    "                f = self.transform(f)\n",
    "            frames.append(f)\n",
    "        video_tensor = torch.stack(frames)\n",
    "        nof = self.df.NumberOfFrames.iloc[idx]\n",
    "        \n",
    "        candidates = nof - self.min_num_frames\n",
    "        start_frame = np.random.choice(candidates)\n",
    "        end_frame = start_frame + self.min_num_frames\n",
    "        sampled_index = np.arange(nof)[start_frame:end_frame:4]\n",
    "        sampled_video_tensor = video_tensor[sampled_index]\n",
    "\n",
    "        ef, esv, edv = self.df.iloc[idx, 1:4]\n",
    "\n",
    "        return sampled_video_tensor, (ef, esv, edv), self.clip_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(num_output_channels=1)\n",
    "])\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "trainset = EchoNetDataset(\n",
    "    root_dir='/home/tienyu/data/EchoNet-Dynamic/Videos',\n",
    "    target_csv='/home/tienyu/data/EchoNet-Dynamic/FileList.csv',\n",
    "    split='train',\n",
    "    transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 1, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "for video_tensor, labels, nofs in trainloader:\n",
    "    print(video_tensor.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/tienyu/data/EchoNet-Dynamic/FileList.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['Split'] == 'TRAIN']\n",
    "validation_df = df[df['Split'] == 'VAL']\n",
    "test_df = df[(df['Split'] == 'TEST') & (df['NumberOfFrames'] > clip_length * sampling_frequency)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1266, 9)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nof = test_df.NumberOfFrames.sample().item()\n",
    "nof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_length = 16\n",
    "sampling_frequency = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([108, 112, 116, 120, 124, 128, 132, 136, 140, 144, 148, 152, 156,\n",
       "       160, 164, 168])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
